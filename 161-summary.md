The rapid progress in artificial intelligence (AI) has prompted the Future of Life Institute, an ngo, to call for a six-month “pause” in the creation of the most advanced forms of AI, including large language models (LLMs), which have surprised even their creators with their unexpected talents as they have been scaled up. LLMs can be trained on the entire internet so that they can perform many white-collar tasks, including writing computer code and summarizing documents. However, their capabilities have raised fears that the technology is advancing too quickly to be safely controlled. Experts are divided on the degree of existential risk posed by AI, but 48% of AI researchers surveyed in 2022 thought there was at least a 10% chance that AI's impact would be "extremely bad (eg, human extinction)." Governments are taking three different approaches to regulation: a “light-touch” approach with no new rules or regulatory bodies, a tougher line with increasingly stringent monitoring and disclosure, and treating AI like medicines with a dedicated regulator, strict testing, and pre-approval before release to the public. The EU’s model is closest to the mark, though its classification system is overwrought, but compelling disclosure about how systems are trained, how they operate, and how they are monitored, and requiring inspections, would be comparable to similar rules in other industries. To monitor the risks posed by AI, governments could form a body modeled on CERN that could also study AI safety and ethics. A measured approach today can provide the foundations on which further rules can be added in the future.